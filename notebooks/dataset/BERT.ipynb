{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCphMk1aQ7BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers datasets -q"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRyvCuF3Qshr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aa563865-65e3-422e-f438-335752868293"
      },
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "import numpy as np \n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "torch.set_grad_enabled(False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version 1.6.0+cu101 available.\n",
            "TensorFlow version 2.3.0 available.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7fe2f0f9d1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcAek65WRizX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "579ab56c-7d06-41dc-c9cc-6f8893755b86"
      },
      "source": [
        "dataset = load_dataset('rotten_tomatoes')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking /root/.cache/huggingface/datasets/b3267cede0118f731406527d730b20a47827f64e15eaece772b81c2ec6c4e129.75128cbb0bbc33ddc2b7299d2a29d743223662ec632601fc16d2dea19bcb1072.py for additional imports.\n",
            "Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/rotten_tomatoes/rotten_tomatoes.py at /root/.cache/huggingface/modules/datasets_modules/datasets/rotten_tomatoes\n",
            "Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/rotten_tomatoes/rotten_tomatoes.py at /root/.cache/huggingface/modules/datasets_modules/datasets/rotten_tomatoes/9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a\n",
            "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/rotten_tomatoes/rotten_tomatoes.py to /root/.cache/huggingface/modules/datasets_modules/datasets/rotten_tomatoes/9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a/rotten_tomatoes.py\n",
            "Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/rotten_tomatoes/dataset_infos.json to /root/.cache/huggingface/modules/datasets_modules/datasets/rotten_tomatoes/9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a/dataset_infos.json\n",
            "Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/rotten_tomatoes/rotten_tomatoes.py at /root/.cache/huggingface/modules/datasets_modules/datasets/rotten_tomatoes/9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a/rotten_tomatoes.json\n",
            "Using custom data configuration default\n",
            "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/rotten_tomatoes/9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a\n",
            "Overwrite dataset info from restored data version.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a\n",
            "Reusing dataset rotten_tomatoes_movie_review (/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a)\n",
            "Constructing Dataset for split train, validation, test, from /root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/9198dbc50858df8bdb0d5f18ccaf33125800af96ad8434bc8b829918c987ee8a\n",
            "100%|██████████| 3/3 [00:00<00:00, 321.34it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8hOdN0oRFt0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0c06129a-8643-4564-fee6-0cf3eb3d38ef"
      },
      "source": [
        "!mkdir repr\n",
        "\n",
        "models = ['bert-base-uncased', 'bert-large-uncased',\n",
        "          'roberta-base', 'roberta-large']\n",
        "splits = ['train', 'validation', 'test']\n",
        "\n",
        "for model_name in models:\n",
        "  for split in splits:\n",
        "    model = AutoModel.from_pretrained(model_name).cuda()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    tokens_pt = tokenizer(\n",
        "        dataset[split]['text'],\n",
        "        return_tensors='pt',\n",
        "        padding='max_length',\n",
        "        max_length=80\n",
        "    )['input_ids'].cuda()\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(tokens_pt, batch_size=512)\n",
        "\n",
        "    repr = []\n",
        "    for review in data_loader:\n",
        "      _, pooled = model(review)\n",
        "      repr.append(pooled)\n",
        "\n",
        "    representation = torch.cat(repr).cpu().numpy()\n",
        "\n",
        "    print(model_name, split, representation.shape)\n",
        "    np.save(f'repr/{model_name}_{split}', representation)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased train (8530, 768)\n",
            "bert-base-uncased validation (1066, 768)\n",
            "bert-base-uncased test (1066, 768)\n",
            "bert-large-uncased train (8530, 1024)\n",
            "bert-large-uncased validation (1066, 1024)\n",
            "bert-large-uncased test (1066, 1024)\n",
            "roberta-base train (8530, 768)\n",
            "roberta-base validation (1066, 768)\n",
            "roberta-base test (1066, 768)\n",
            "roberta-large train (8530, 1024)\n",
            "roberta-large validation (1066, 1024)\n",
            "roberta-large test (1066, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvKcJfzDZGSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e171c9b1-1835-48fe-feff-1dd3d04b148a"
      },
      "source": [
        "!zip -r repr.zip repr"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: repr/ (stored 0%)\n",
            "  adding: repr/bert-base-uncased_test.npy (deflated 10%)\n",
            "  adding: repr/bert-base-uncased_validation.npy (deflated 10%)\n",
            "  adding: repr/roberta-base_test.npy (deflated 8%)\n",
            "  adding: repr/roberta-base_train.npy (deflated 8%)\n",
            "  adding: repr/bert-large-uncased_train.npy (deflated 10%)\n",
            "  adding: repr/bert-large-uncased_test.npy (deflated 10%)\n",
            "  adding: repr/roberta-base_validation.npy (deflated 8%)\n",
            "  adding: repr/roberta-large_train.npy (deflated 8%)\n",
            "  adding: repr/roberta-large_test.npy (deflated 8%)\n",
            "  adding: repr/bert-base-uncased_train.npy (deflated 10%)\n",
            "  adding: repr/bert-large-uncased_validation.npy (deflated 10%)\n",
            "  adding: repr/roberta-large_validation.npy (deflated 8%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF82cqqhaSP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}